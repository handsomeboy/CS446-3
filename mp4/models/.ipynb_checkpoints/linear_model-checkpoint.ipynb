{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Linear model base class.\"\"\"\n",
    "\n",
    "import abc\n",
    "import numpy as np\n",
    "import six\n",
    "\n",
    "\n",
    "@six.add_metaclass(abc.ABCMeta)\n",
    "class LinearModel(object):\n",
    "    \"\"\"Abstract class for linear models.\"\"\"\n",
    "\n",
    "    def __init__(self, ndims, w_init='zeros', w_decay_factor=0.001):\n",
    "        \"\"\"Initialize a linear model.\n",
    "\n",
    "        This function prepares an uninitialized linear model.\n",
    "        It will initialize the weight vector, self.w, based on the method\n",
    "        specified in w_init.\n",
    "\n",
    "        We assume that the last index of w is the bias term, self.w = [w,b]\n",
    "\n",
    "        self.w(numpy.ndarray): array of dimension (n_dims+1,1)\n",
    "\n",
    "        w_init needs to support:\n",
    "          'zeros': initialize self.w with all zeros.\n",
    "          'ones': initialze self.w with all ones.\n",
    "          'uniform': initialize self.w with uniform random number between [0,1)\n",
    "\n",
    "        Args:\n",
    "            ndims(int): feature dimension\n",
    "            w_init(str): types of initialization.\n",
    "            w_decay_factor(float): Weight decay factor.\n",
    "        \"\"\"\n",
    "        self.ndims = ndims\n",
    "        self.w_init = w_init\n",
    "        self.w_decay_factor = w_decay_factor\n",
    "        self.w = np.zeros((self.ndims+1,1))\n",
    "        if w_init == 'ones':\n",
    "            self.w = np.ones((self.ndims+1,1))\n",
    "        elif w_init == 'uniform':\n",
    "            self.w = np.random.uniform((self.ndims+1,1))\n",
    "        self.x = None\n",
    "        # Implementation here.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward operation for linear models.\n",
    "\n",
    "        Performs the forward operation. Appends 1 to x then compute\n",
    "        f=w^Tx, and return f.\n",
    "\n",
    "        Args:\n",
    "            x(numpy.ndarray): Dimension of (N, ndims), N is the number\n",
    "              of examples.\n",
    "\n",
    "        Returns:\n",
    "            (numpy.ndarray): Dimension of (N,1)\n",
    "        \"\"\"\n",
    "        # Implementation here.\n",
    "        self.x = x\n",
    "        b = np.ones((len(self.x),1))\n",
    "        self.x = np.concatenate([self.x,b], axis=1)\n",
    "        f = np.dot(self.x, self.w)\n",
    "\n",
    "        return f\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, f, y):\n",
    "        \"\"\"Do not need to be implemented here.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def total_loss(self, f, y):\n",
    "        \"\"\"Do not need to be implemented here.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, f):\n",
    "        \"\"\"Do not need to be implemented here.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Implements support vector machine.\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "from linear_model import LinearModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SupportVectorMachine(LinearModel):\n",
    "    \"\"\"Implements a linear regression mode model\"\"\"\n",
    "\n",
    "    def backward(self, f, y):\n",
    "        \"\"\"Performs the backward operation based on the loss in total_loss.\n",
    "\n",
    "        By backward operation, it means to compute the gradient of the loss\n",
    "        w.r.t w.\n",
    "\n",
    "        Hint: You may need to use self.x, and you made need to change the\n",
    "        forward operation.\n",
    "\n",
    "        Args:\n",
    "            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n",
    "            y(numpy.ndarray): Ground truth label, dimension (N,1).\n",
    "        Returns:\n",
    "            total_grad(numpy.ndarray): Gradient of L w.r.t to self.w,\n",
    "              dimension (ndims+1, 1).\n",
    "        \"\"\"\n",
    "        \n",
    "        reg_grad = None\n",
    "        loss_grad = None\n",
    "        # Implement here\n",
    "        reg_grad = self.w_decay_factor * self.w\n",
    "        unit = np.maximum(0, np.sign(1-np.multiply(y,f)))\n",
    "        print(unit)\n",
    "        print(np.multiply(y,f).shape)\n",
    "        loss_grad = - np.dot(np.transpose(self.x), y * unit)\n",
    "        #print(loss_grad.shape)\n",
    "        #print(reg_grad.shape)\n",
    "        total_grad = reg_grad + loss_grad\n",
    "        return total_grad\n",
    "\n",
    "    def total_loss(self, f, y):\n",
    "        \"\"\"The sum of the loss across batch examples + L2 regularization.\n",
    "        Total loss is hinge_loss + w_decay_factor/2*||w||^2\n",
    "\n",
    "        Args:\n",
    "            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n",
    "            y(numpy.ndarray): Ground truth label, dimension (N,1).\n",
    "        Returns:\n",
    "            total_loss (float): sum hinge loss + reguarlization.\n",
    "        \"\"\"\n",
    "\n",
    "        hinge_loss = None\n",
    "        l2_loss = None\n",
    "        # Implementation here.\n",
    "        hinge_loss = np.sum(np.maximum(0. , 1. - np.multiply(y,f)))\n",
    "        l2_loss = 0.5 * self.w_decay_factor * np.norm(self.w)**2\n",
    "        total_loss = hinge_loss + l2_loss\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, f):\n",
    "        \"\"\"Converts score to prediction.\n",
    "\n",
    "        Args:\n",
    "            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n",
    "        Returns:\n",
    "            (numpy.ndarray): Hard predictions from the score, f,\n",
    "              dimension (N,1). Tie break 0 to 1.0.\n",
    "        \"\"\"\n",
    "        # Implementation here.\n",
    "\n",
    "        y_predict = np.array([1 if f[i] >= 0 else -1 for i in range(len(f))]).reshape(-1,1)\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(11, 1)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "w_decay_factor = 0.001\n",
    "num_steps = 5000\n",
    "opt_method = 'iter'\n",
    "feature_type = 'default'\n",
    "\n",
    "# Load dataset and data processing.\n",
    "#train_set = read_dataset(\"../data/train.txt\", \"../data/image_data/\")\n",
    "#train_set = preprocess_data(train_set, feature_type)\n",
    "\n",
    "    # Initialize model.\n",
    "ndim = 10#train_set['image'][0].shape[0]\n",
    "print(ndim)\n",
    "model = SupportVectorMachine(ndim, 'ones', w_decay_factor=w_decay_factor)\n",
    "print(model.w.shape)\n",
    "print(model.ndims)\n",
    "# print(model.w)\n",
    "#print(train_set['label'])\n",
    "x= np.ones((2, ndim))\n",
    "y = -np.ones((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 1.]]\n",
      "(2, 1)\n",
      "(11, 1)\n",
      "(11, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001],\n",
       "       [ 2.001]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(f, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
