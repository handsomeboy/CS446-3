{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define the number of iterations.\n",
    "num_itr = 1000\n",
    "\n",
    "# define batch size.\n",
    "batchSize = 3.\n",
    "\n",
    "# define the input data dimension.\n",
    "inputSize = 2\n",
    "\n",
    "# define the output dimension.\n",
    "outputSize = 1\n",
    "\n",
    "# define the dimension of the hidden layer.\n",
    "hiddenSize = 3\n",
    "\n",
    "\n",
    "class Neural_Network():\n",
    "    def __init__(self):     \n",
    "        #weights\n",
    "        self.U = np.random.randn(inputSize, hiddenSize) \n",
    "        self.W = np.random.randn(hiddenSize, outputSize) \n",
    "        self.e = np.random.randn(hiddenSize) \n",
    "        self.f = np.random.randn(outputSize) \n",
    "\n",
    "\n",
    "    def fully_connected(self, X, U, e):\n",
    "        '''\n",
    "        fully connected layer.\n",
    "        inputs:\n",
    "            U: weight \n",
    "            e: bias\n",
    "        outputs:\n",
    "            X * U + e\n",
    "        '''\n",
    "        return np.dot(X, U) + e\n",
    "\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        '''\n",
    "        sigmoid activation function. \n",
    "        inputs: s\n",
    "        outputs: sigmoid(s)  \n",
    "        '''\n",
    "        return 1/(1+np.exp(-s))\n",
    "\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        '''\n",
    "        derivative of sigmoid (Written section, Part a).\n",
    "        inputs: \n",
    "            s = sigmoid(x)\n",
    "        outputs: \n",
    "            derivative sigmoid(x) as a function of s \n",
    "        '''\n",
    "        d_sigmoid = self.sigmoid(s)*(1-self.sigmoid(s))\n",
    "        return d_sigmoid\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        forward propagation through the network.\n",
    "        inputs:\n",
    "            X: input data (batchSize, inputSize) \n",
    "        outputs:\n",
    "            c: output (batchSize, outputSize)\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.z = self.fully_connected(self.X, self.U, self.e)\n",
    "        self.b = self.sigmoid(self.z)\n",
    "        self.h = self.fully_connected(self.b, self.W, self.f)\n",
    "        self.c = self.sigmoid(self.h)\n",
    "        return c\n",
    "\n",
    "\n",
    "    def d_loss_o(self, gt, o):\n",
    "        '''\n",
    "        computes the derivative of the L2 loss with respect to \n",
    "        the network's output.\n",
    "        inputs:\n",
    "            gt: ground-truth (batchSize, outputSize)\n",
    "            o: network output (batchSize, outputSize)\n",
    "        outputs:\n",
    "            d_o: derivative of the L2 loss with respect to the network's \n",
    "            output o. (batchSize, outputSize)\n",
    "        '''\n",
    "        return d_o\n",
    "\n",
    "\n",
    "    def error_at_layer2(self, d_o, o):\n",
    "        '''\n",
    "        computes the derivative of the loss with respect to layer2's output\n",
    "        (Written section, Part b).\n",
    "        inputs:\n",
    "            d_o: derivative of the loss with respect to the network output (batchSize, outputSize)\n",
    "            o: the network output (batchSize, outputSize)\n",
    "        returns \n",
    "            delta_k: the derivative of the loss with respect to the output of the second\n",
    "            fully connected layer (batchSize, outputSize).\n",
    "        '''\n",
    "        return delta_k\n",
    "\n",
    "\n",
    "    def error_at_layer1(self, delta_k, W, b):\n",
    "        '''\n",
    "        computes the derivative of the loss with respect to layer1's output (Written section, Part e).\n",
    "        inputs:\n",
    "            delta_k: derivative of the loss with respect to the output of the second\n",
    "            fully connected layer (batchSize, outputSize). \n",
    "            W: the weights of the second fully connected layer (hiddenSize, outputSize).\n",
    "            b: the input to the second fully connected layer (batchSize, hiddenSize).\n",
    "        returns:\n",
    "            delta_j: the derivative of the loss with respect to the output of the second\n",
    "            fully connected layer (batchSize, hiddenSize).\n",
    "        '''\n",
    "        return delta_j\n",
    "\n",
    "\n",
    "    def derivative_of_w(self, b, delta_k):\n",
    "        '''\n",
    "        computes the derivative of the loss with respect to W (Written section, Part c).\n",
    "        inputs:\n",
    "            b: the input to the second fully connected layer (batchSize, hiddenSize).\n",
    "            delta_k: the derivative of the loss with respect to the output of the second\n",
    "            fully connected layer's output (batchSize, outputSize).\n",
    "        returns:\n",
    "            d_w: the derivative of loss with respect to W  (hiddenSize ,outputSize).\n",
    "        '''\n",
    "        return d_w\n",
    "\n",
    "\n",
    "    def derivative_of_u(self, X, delta_j):\n",
    "        '''\n",
    "        computes the derivative of the loss with respect to U (Written section, Part f).\n",
    "        inputs:\n",
    "            X: the input to the network (batchSize, inputSize).\n",
    "            delta_j: the derivative of the loss with respect to the output of the first\n",
    "            fully connected layer's output (batchSize, hiddenSize).\n",
    "        returns:\n",
    "            d_u: the derivative of loss with respect to U (inputSize, hiddenSize).\n",
    "        '''\n",
    "        return d_u\n",
    "\n",
    "\n",
    "    def derivative_of_e(self, delta_j):\n",
    "        '''\n",
    "        computes the derivative of the loss with respect to e (Written section, Part g).\n",
    "        inputs:\n",
    "            delta_j: the derivative of the loss with respect to the output of the first\n",
    "            fully connected layer's output (batchSize, hiddenSize).\n",
    "        returns:\n",
    "            d_e: the derivative of loss with respect to e (hiddenSize).\n",
    "        '''\n",
    "        return d_e\n",
    "\n",
    "\n",
    "    def derivative_of_f(self, delta_k):\n",
    "        '''\n",
    "        computes the derivative of the loss with respect to f (Written section, Part d).\n",
    "        inputs:\n",
    "            delta_k: the derivative of the loss with respect to the output of the second\n",
    "            fully connected layer's output (batchSize, outputSize).\n",
    "        returns:\n",
    "            d_f: the derivative of loss with respect to f (outputSize).\n",
    "        '''\n",
    "        return d_f\n",
    "\n",
    "\n",
    "    def backward(self, X, gt, o):\n",
    "        '''\n",
    "        backpropagation through the network.\n",
    "        Task: perform the 8 steps required below.\n",
    "        inputs: \n",
    "            X: input data (batchSize, inputSize)\n",
    "            y: ground truth (batchSize, outputSize)\n",
    "            o: network output (batchSize, outputSize)        \n",
    "        '''\n",
    "\n",
    "        # 1. Compute the derivative of the loss with respect to c.\n",
    "        # Call: d_loss_o\n",
    "        \n",
    "\n",
    "        # 2. Compute the error at the second layer (Written section, Part b).\n",
    "        # Call: error_at_layer2\n",
    "        \n",
    "\n",
    "        # 3. Compute the derivative of W (Written section, Part c).\n",
    "        # Call: derivative_of_w\n",
    "        \n",
    "\n",
    "        # 4. Compute the derivative of f (Written section, Part d).\n",
    "        # Call: derivative_of_f\n",
    "        \n",
    "\n",
    "        # 5. Compute the error at the first layer (Written section, Part e).\n",
    "        # Call: error_at_layer1 \n",
    "        \n",
    "\n",
    "        # 6. Compute the derivative of U (Written section, Part f).\n",
    "        # Call: derivative_of_u\n",
    "        \n",
    "\n",
    "        # 7. Compute the derivative of e (Written section, Part g).\n",
    "        # Call: derivative_of_e\n",
    "           \n",
    "\n",
    "        # 8. Update the parameters\n",
    "        \n",
    "        \n",
    "\n",
    "    def train (self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[[ 1.          0.55555556]\n",
      " [ 0.14285714  1.        ]\n",
      " [ 0.          0.66666667]]\n",
      "Actual Output: \n",
      "[[ 0.59]\n",
      " [ 0.6 ]\n",
      " [ 0.19]]\n",
      "Predicted Output: \n",
      "[[ 0.51077249]\n",
      " [ 0.43333041]\n",
      " [ 0.48185595]]\n",
      "Loss: \n",
      "0.0397452154692\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (3,1) not aligned: 1 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ba304f28c43b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-ba304f28c43b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: \\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9210c7d1d9f0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9210c7d1d9f0>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X, gt, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;31m# 2. Compute the error at the second layer (Written section, Part b).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;31m# Call: error_at_layer2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mdelta_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_at_layer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9210c7d1d9f0>\u001b[0m in \u001b[0;36merror_at_layer2\u001b[1;34m(self, d_o, o)\u001b[0m\n\u001b[0;32m    102\u001b[0m         '''\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mdelta_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdelta_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,) and (3,1) not aligned: 1 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" Main function \"\"\"\n",
    "    # generate random input data of dimension (batchSize, inputSize). \n",
    "    a = np.random.randint(0, high=10, size=[3,2], dtype='l')\n",
    "\n",
    "    # generate random ground truth.\n",
    "    t = np.random.randint(0, high=100, size=[3,1], dtype='l')\n",
    "\n",
    "    # scale the input and output data.\n",
    "    a = a/np.amax(a, axis=0) \n",
    "    t = t/100 \n",
    "\n",
    "    # create an instance of Neural_Network.\n",
    "    NN = Neural_Network()\n",
    "    for i in range(num_itr): \n",
    "        print(\"Input: \\n\" + str(a)) \n",
    "        print(\"Actual Output: \\n\" + str(t))\n",
    "        print(\"Predicted Output: \\n\" + str(NN.forward(a)))\n",
    "        print(\"Loss: \\n\" + str(np.mean(np.square(t - NN.forward(a)))))\n",
    "        print(\"\\n\")\n",
    "        NN.train(a, t)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
