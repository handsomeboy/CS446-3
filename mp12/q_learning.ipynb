{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import pong_game as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Game name.\n",
    "GAME = 'Pong'\n",
    "\n",
    "# Number of valid actions.\n",
    "ACTIONS = 3\n",
    "\n",
    "# Decay rate of past observations.\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Timesteps to observe before training.\n",
    "OBSERVE = 5000.\n",
    "\n",
    "# Frames over which to anneal epsilon.\n",
    "EXPLORE = 5000.\n",
    "\n",
    "# Final value of epsilon.\n",
    "FINAL_EPSILON = 0.05\n",
    "\n",
    "# Starting value of epsilon.\n",
    "INITIAL_EPSILON = 1.0\n",
    "\n",
    "# Number of previous transitions to remember in the replay memory.\n",
    "REPLAY_MEMORY = 590000\n",
    "\n",
    "# Size of minibatch.\n",
    "BATCH = 32\n",
    "\n",
    "# Only select an action every Kth frame, repeat the same action\n",
    "# for other frames.\n",
    "K = 2\n",
    "\n",
    "# Learning Rate.\n",
    "Lr = 1e-6\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\" Initializa the weight variable.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\" Initializa the bias variable.\"\"\"\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    \"\"\" Define a convolutional layer.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\" Define a maxpooling layer.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def createNetwork():\n",
    "    \"\"\" Create a convolutional network for estimating the Q value.\n",
    "    Args:\n",
    "    Returns:\n",
    "        s: Input layer\n",
    "        readout: Output layer with the Q-values for every possible action\n",
    "    \"\"\"\n",
    "    # Initialize the network weights and biases.\n",
    "    W_conv1 = weight_variable([8, 8, 4, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    W_conv2 = weight_variable([4, 4, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "\n",
    "    W_fc1 = weight_variable([1600, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "\n",
    "    W_fc2 = weight_variable([512, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "\n",
    "    # Input layer.\n",
    "    s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "    # Hidden layers.\n",
    "    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Output layer\n",
    "    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "    return s, readout\n",
    "\n",
    "\n",
    "def get_action_index(readout_t, epsilon, t):\n",
    "    \"\"\" Choose an action epsilon-greedily.\n",
    "    Details:\n",
    "        choose an action randomly:\n",
    "        (1) in the observation phase (t<OBSERVE).\n",
    "        (2) beyond the observation phase with probability \"epsilon\".\n",
    "        otherwise, choose the action with the highest Q-value.\n",
    "    Args:\n",
    "        readout_t: a vector with the Q-value associated with every action.\n",
    "        epsilon: tempreture variable for exploration-exploitation.\n",
    "        t: current number of iterations.\n",
    "    Returns:\n",
    "        index: the index of the action to be taken next.\n",
    "    \"\"\"\n",
    "\n",
    "    action_index = 0\n",
    "    if t < OBSERVE:\n",
    "        action_index = np.random.randint(0,3)\n",
    "    else:\n",
    "        a_eps = np.random.randint(0,3)\n",
    "        a_t = np.argmax(readout_t)\n",
    "        action_index = np.random.choice([a_eps, a_t],p=[epsilon, 1-epsilon])\n",
    "\n",
    "    return action_index\n",
    "\n",
    "\n",
    "def scale_down_epsilon(epsilon, t):\n",
    "    \"\"\" Decrease epsilon after by ((INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE )\n",
    "    in case epsilon is larger than the desired final epsilon or beyond\n",
    "    the observation phase.\n",
    "    Args:\n",
    "        epsilon: the current value of epsilon.\n",
    "        t: current number of iterations.\n",
    "    Returns:\n",
    "        the updated epsilon\n",
    "    \"\"\"\n",
    "    if t > OBSERVE and epsilon > FINAL_EPSILON:\n",
    "        epsilon -= ((INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE )\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "def run_selected_action(a_t, s_t, game_state):\n",
    "    \"\"\" Run the selected action and return the next state and reward.\n",
    "    Do not forget that state is composed of the 4 previous frames.\n",
    "    Hint: check the initialization for the interface to the game simulator.\n",
    "    Args:\n",
    "        a_t: current action.\n",
    "        s_t: current state.\n",
    "        game_state: game state to communicate with emulator.\n",
    "    Returns:\n",
    "        s_t1: next state.\n",
    "        r_t: reward.\n",
    "        terminal: indicating whether the episode terminated (output of the simulator).\n",
    "    \"\"\"\n",
    "    \n",
    "    x_t, r_t, terminal = game_state.frame_step(at)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)\n",
    "    s_t1 = np.stack((st[:,:,1],st[:,:,2], st[:,:,3],x_t), axis=2) \n",
    "    \n",
    "    return s_t1, r_t, terminal\n",
    "\n",
    "\n",
    "def compute_cost(target_q, a_t, q_value):\n",
    "    \"\"\" Compute the cost.\n",
    "    Args:\n",
    "        target_q: target Q-value.\n",
    "        a_t: current action.\n",
    "        q_value: current Q-value.\n",
    "    Returns:\n",
    "        cost\n",
    "    \"\"\"\n",
    "    # Q-value for the action.\n",
    "    readout_action = tf.reduce_sum(tf.multiply(q_value, a_t), reduction_indices=1)\n",
    "\n",
    "    # Q-Learning Cost.\n",
    "    cost = tf.reduce_mean(tf.square(target_q - readout_action))\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def compute_target_q(r_batch, readout_j1_batch, terminal_batch):\n",
    "    \"\"\" Compute the target Q-value for all samples in the batch.\n",
    "    Distinguish two cases:\n",
    "    1. The next state is a terminal state.\n",
    "    2. The next state is not a terminal state.\n",
    "    Args:\n",
    "        r_batch: batch of rewards.\n",
    "        readout_j1_batch: batch of Q-values associated with the next state.\n",
    "        terminal_batch: batch of boolean variables indicating the game termination.\n",
    "    Returns:\n",
    "        target_q_batch: batch of target Q values.\n",
    "\n",
    "    Hint: distinguish two cases: (1) terminal state and (2) non terminal states\n",
    "    \"\"\"\n",
    "\n",
    "    target_q_batch = []\n",
    "\n",
    "    for i in range(0, len(terminal_batch)):\n",
    "        # If the terminal state is reached, the Q-value is only equal to the reward.\n",
    "        if terminal_batch[i] ==False:\n",
    "            target_q_batch.append(r_batch[i]+GAMMA*np.max(readout_j1_batch[i]))\n",
    "            \n",
    "        elif terminal_batch[i] ==True:\n",
    "            target_q_batch.append(r_batch[i])\n",
    "        \n",
    "\n",
    "    return target_q_batch\n",
    "\n",
    "\n",
    "def trainNetwork(s, readout, sess):\n",
    "    \"\"\" Train the artificial agent using Q-learning to play the pong game.\n",
    "    Args:\n",
    "        s: the current state formed by 4 frames of the playground.\n",
    "        readout: the Q value for each passible action in the current state.\n",
    "        sess: session\n",
    "    \"\"\"\n",
    "\n",
    "    # Placeholder for the action.\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "\n",
    "    # Placeholder for the target Q value.\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "\n",
    "    # Compute the loss.\n",
    "    cost = compute_cost(y, a, readout)\n",
    "\n",
    "    # Training operation.\n",
    "    train_step = tf.train.AdamOptimizer(Lr).minimize(cost)\n",
    "\n",
    "    # Open up a game state to communicate with emulator.\n",
    "    game_state = game.GameState()\n",
    "\n",
    "    # Initialize the replay memory.\n",
    "    D = deque()\n",
    "\n",
    "    # Initialize the action vector.\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "\n",
    "    # Initialize the state of the game.\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    # Save and load model checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks_q_learning\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    # Initialize the epsilon value for the exploration phase.\n",
    "    epsilon = INITIAL_EPSILON\n",
    "\n",
    "    # Initialize the iteration counter.\n",
    "    t = 0\n",
    "\n",
    "    while True:\n",
    "        # Choose an action epsilon-greedily.\n",
    "        readout_t = readout.eval(feed_dict={s: [s_t]})[0]\n",
    "\n",
    "        action_index = get_action_index(readout_t, epsilon, t)\n",
    "\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "\n",
    "        a_t[action_index] = 1\n",
    "\n",
    "        # Scale down epsilon during the exploitation phase.\n",
    "        epsilon = scale_down_epsilon(epsilon, t)\n",
    "\n",
    "        # Run the selected action and update the replay memeory\n",
    "        for i in range(0, K):\n",
    "            # Run the selected action and observe next state and reward.\n",
    "            s_t1, r_t, terminal = run_selected_action(a_t, s_t, game_state)\n",
    "\n",
    "            # Store the transition in the replay memory D.\n",
    "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "        # Start training once the observation phase is over.\n",
    "        if (t > OBSERVE):\n",
    "\n",
    "            # Sample a minibatch to train on.\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "\n",
    "            # Get the batch variables.\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "            terminal_batch = [d[4] for d in minibatch]\n",
    "\n",
    "            # Compute the target Q-Value\n",
    "            readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})\n",
    "            target_q_batch = compute_target_q(r_batch, readout_j1_batch, terminal_batch)\n",
    "\n",
    "            # Perform gradient step.\n",
    "            train_step.run(feed_dict={\n",
    "                y: target_q_batch,\n",
    "                a: a_batch,\n",
    "                s: s_j_batch})\n",
    "\n",
    "        # Update the state.\n",
    "        s_t = s_t1\n",
    "\n",
    "        # Update the number of iterations.\n",
    "        t += 1\n",
    "\n",
    "        # Save a checkpoint every 10000 iterations.\n",
    "        if t % 10000 == 0:\n",
    "            saver.save(sess, 'saved_networks_q_learning/' + GAME + '-dqn', global_step=t)\n",
    "\n",
    "        # Print info.\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \"/ Q_MAX %e\" % np.max(readout_t))\n",
    "\n",
    "\n",
    "def playGame():\n",
    "    \"\"\"Paly the pong game\"\"\"\n",
    "\n",
    "    # Start an active session.\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Create the network.\n",
    "    s, readout = createNetwork()\n",
    "\n",
    "    # Q-Learning\n",
    "    s, readout = trainNetwork(s, readout, sess)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function \"\"\"\n",
    "    playGame()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
