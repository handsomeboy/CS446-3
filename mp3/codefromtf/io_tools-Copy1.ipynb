{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Input and output helpers to load in data.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def read_dataset(path_to_dataset_folder,index_filename):\n",
    "    \"\"\" Read dataset into numpy arrays with preprocessing included\n",
    "    Args:\n",
    "        path_to_dataset_folder(str): path to the folder containing samples and indexing.txt\n",
    "        index_filename(str): indexing.txt\n",
    "    Returns:\n",
    "        A(numpy.ndarray): sample feature matrix A = [[1, x1], \n",
    "                                                     [1, x2], \n",
    "                                                     [1, x3],\n",
    "                                                     .......] \n",
    "                                where xi is the 16-dimensional feature of each sample\n",
    "            \n",
    "        T(numpy.ndarray): class label vector T = [y1, y2, y3, ...] \n",
    "                             where yi is +1/-1, the label of each sample \n",
    "    \"\"\"\n",
    "    with open(path_to_dataset_folder+'/'+index_filename, 'r') as f:\n",
    "        label_sample_path = f.readlines()\n",
    "    T = np.array([max(0,float(label_sample_path[i].split(' ')[0])) for i in range(len(label_sample_path))])\n",
    "    sample_path = [label_sample_path[i].split(' ')[1].replace('\\n','') for i in range(len(label_sample_path))]\n",
    "    \n",
    "    A = []\n",
    "    for i in range(len(sample_path)):\n",
    "        with open(path_to_dataset_folder+'/'+sample_path[i], 'r') as f:\n",
    "            row_data = f.read().strip().split('  ')\n",
    "            A.append([1.  if i ==0 else float(row_data[i-1]) for i in range(len(row_data)+1)])\n",
    "    A = np.array(A)\n",
    "    \n",
    "        \n",
    "    \n",
    "    return A, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y_true = read_dataset('C:/Users/PIxel/CS446/mp3/data/trainset','indexing.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"logistic model class for binary classification.\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class LogisticModel_TF(object):\n",
    "    \n",
    "    def __init__(self, ndims, W_init='zeros'):\n",
    "        \"\"\"Initialize a logistic model.\n",
    "\n",
    "        This function prepares an initialized logistic model.\n",
    "        It will initialize the weight vector, self.W, based on the method\n",
    "        specified in W_init.\n",
    "\n",
    "        We assume that the FIRST index of Weight is the bias term, \n",
    "            Weight = [Bias, W1, W2, W3, ...] \n",
    "            where Wi correspnds to each feature dimension\n",
    "\n",
    "        W_init needs to support:\n",
    "          'zeros': initialize self.W with all zeros.\n",
    "          'ones': initialze self.W with all ones.\n",
    "          'uniform': initialize self.W with uniform random number between [0,1)\n",
    "          'gaussian': initialize self.W with gaussion distribution (0, 0.1)\n",
    "\n",
    "        Args:\n",
    "            ndims(int): feature dimension\n",
    "            W_init(str): types of initialization.\n",
    "        \"\"\"\n",
    "        self.ndims = ndims\n",
    "        self.W_init = W_init\n",
    "        self.W0 = None\n",
    "        ###############################################################\n",
    "        # Fill your code below\n",
    "        ###############################################################\n",
    "        if W_init == 'zeros':\n",
    "            # Hint: self.W0 = tf.zeros([self.ndims+1,1])\n",
    "            self.W0 = tf.zeros([self.ndims, 1])\n",
    "        elif W_init == 'ones':\n",
    "            self.W0 = tf.ones([self.ndims, 1])\n",
    "        elif W_init == 'uniform':\n",
    "            self.W0 = tf.random_uniform([self.ndims, 1], maxval=1)\n",
    "        elif W_init == 'gaussian':\n",
    "            self.W0 = tf.random_normal([self.ndims, 1],mean=0.0,stddev=0.1)\n",
    "        else:\n",
    "            print ('Unknown W_init ', W_init) \n",
    "        #self.graph = tf.Graph()\n",
    "        \n",
    "    def build_graph(self, learn_rate, Y_true, X):\n",
    "        \"\"\" build tensorflow training graph for logistic model.\n",
    "        Args:\n",
    "            learn_rate: learn rate for gradient descent\n",
    "            ......: append as many arguments as you want\n",
    "        \"\"\"\n",
    "        ###############################################################\n",
    "        # Fill your code in this function\n",
    "        ###############################################################\n",
    "        # Hint: self.W = tf.Variable(self.W0)\n",
    "        self.W = tf.Variable(self.W0)\n",
    "        self.lr = learn_rate\n",
    "        self.X_TF = X \n",
    "        self.y_TF = np.array([Y_true]).T.astype(int) \n",
    "        self.predictions = tf.sigmoid(tf.matmul(tf.cast(self.X_TF,tf.float32), self.W))\n",
    "        self.classify = tf.cond(tf.less(0.5, self.predictions),0,1)\n",
    "        self.cost = tf.reduce_mean(tf.square(tf.subtract(tf.cast(self.y_TF,tf.float32), self.predictions)))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.cost)        \n",
    "\n",
    "        pass\n",
    "        \n",
    "    def fit(self, Y_true, X, max_iters, learn_rate):\n",
    "        \"\"\" train model with input dataset using gradient descent. \n",
    "        Args:\n",
    "            Y_true(numpy.ndarray): dataset labels with a dimension of (# of samples,1)\n",
    "            X(numpy.ndarray): input dataset with a dimension of (# of samples, ndims+1)\n",
    "            max_iters: maximal number of training iterations\n",
    "            ......: append as many arguments as you want\n",
    "        Returns:\n",
    "            (numpy.ndarray): sigmoid output from well trained logistic model, used for classification\n",
    "                             with a dimension of (# of samples, 1)\n",
    "        \"\"\"\n",
    "        ###############################################################\n",
    "        # Fill your code in this function\n",
    "        ###############################################################\n",
    "        def accuracy(Y_t, Y_p):\n",
    "            acc_vec = np.array([1 if Y_t[i] == Y_p[i] else 0 for i in range(len(Y_p))])\n",
    "            acc_val = np.mean(acc_vec)\n",
    "            return acc_vec, acc_val\n",
    "        \n",
    "        self.build_graph(learn_rate, Y_true, X)\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            Y = np.array([Y_true]).T.astype(int)    \n",
    "            for epoch in range(max_iters):\n",
    "                sess.run(self.optimizer)\n",
    "                if epoch % 200 == 0:\n",
    "                    classify = sess.run(self.classify)\n",
    "                    acc_vec, acc_val = accuracy(Y, classify)\n",
    "                    cost = sess.run(self.cost)\n",
    "                    \n",
    "                    print(epoch, '..', cost,acc_val)\n",
    "                if epoch+1 == max_iters:\n",
    "                    classify = sess.run(self.classify)\n",
    "                    acc_vec, acc_val = accuracy(Y, classify)\n",
    "                    print(\"Final step accuracy:\", acc_val)\n",
    "                    return acc_vec\n",
    "                    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model =LogisticModel_TF(17,'zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "true_fn must be callable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ac63f1a83215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc_ve\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-74a16f0966ab>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, Y_true, X, max_iters, learn_rate)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0macc_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-74a16f0966ab>\u001b[0m in \u001b[0;36mbuild_graph\u001b[1;34m(self, learn_rate, Y_true, X)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_TF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY_true\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_TF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mless\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_TF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             instructions)\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    291\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[1;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[0;32m   1793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1795\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"true_fn must be callable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1796\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1797\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"false_fn must be callable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: true_fn must be callable."
     ]
    }
   ],
   "source": [
    "acc_ve = model.fit(Y_true, X,2000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(acc_ve[10:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645.0\n",
      "[ 0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(1290/2)\n",
    "print(Y_true[770:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
